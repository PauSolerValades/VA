{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "==============================================================================================\n",
    "# Practicum 6 - Part 2\n",
    "\n",
    "=============================================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delivery\n",
    "\n",
    "Up to **1 point out of 10** will be penalized if the following requirements are not fulfilled:\n",
    "\n",
    "- Implemented code should be commented.\n",
    "\n",
    "- The questions introduced in the exercises must be answered.\n",
    "\n",
    "- Add title to the figures to explain what is displayed.\n",
    "\n",
    "- Comments need to be in **english**.\n",
    "\n",
    "- The deliverable must be a file named **P6_Student1_Student2.zip** that includes:\n",
    "    - The notebook P6_2_Student1_Student2.ipynb completed with the solutions to the exercises and their corresponding comments.\n",
    "    - All the images used in this notebook.\n",
    "\n",
    "**Deadline (Group A- Group F): December 23th, 23:00 h**\n",
    "\n",
    "**Deadline (Group B): December 24th, 23:00 h**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "==============================================================================================\n",
    "## Dimensionality reduction and face detection\n",
    "==============================================================================================\n",
    "\n",
    "**Problem to solve**: Define a space of image feature that allows to represent objects based on their appearance or a set of local features in the image.\n",
    "\n",
    "Documentation [Face recognition skimage](https://scikit-learn.org/stable/auto_examples/applications/plot_face_recognition.html)\n",
    "\n",
    "==============================================================================================\n",
    "\n",
    "\n",
    "Today's exercices will practise the following:\n",
    "\n",
    "1. Define an appropiate representation (descriptors objects):\n",
    "    - Normally, reduce size of the data preserving the invariance and removing redundant dimensions.\n",
    "\n",
    "\n",
    "2. Train a classifier from a set of examples with their descriptors.\n",
    "\n",
    "\n",
    "3. Recognize a new face example using the learned model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "\n",
    "import numpy as np\n",
    "from numpy import unique\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal Component Analysis (PCA)\n",
    "\n",
    "### Dimensionality and redundancy\n",
    "\n",
    "Imagine we have a dataset with 100x100 pixel images, it means we have 10000 dimensions. We want to construct a low-dimensional linear subspace that best explains the variation in the set of face images (**Eigenfaces space**)\n",
    "\n",
    "<img src=\"notebook_images/subspace.PNG\" width=200, height=200>\n",
    "\n",
    "Each image has m rows and n columns and defines a vector of (mxn) elements. We need to choose the most valuable pixels in order to avoid compute all dimensions. \n",
    "\n",
    "<img src=\"notebook_images/feature_vector.PNG\" width=800, height=400>\n",
    "\n",
    "We look for a transformation of the original space to a smaller (M << (mxn)) where faces are represented with their coordinates in this new space R.\n",
    "\n",
    "To reduce the dimensionality retaining the information necessary to classify and recognize, we are going to use the **Eigenfaces method** \n",
    "\n",
    "### How to build a reduced space?\n",
    "\n",
    "To build this new space, we are going to use the **Principal Component Analysis**. Given a large space, the PCA looks for the minimum number of axes that best represents the variation of the data.\n",
    "\n",
    "<img src=\"notebook_images/pca.PNG\" width=400, height=400>\n",
    "\n",
    "The eigenvectors of the covariance matrix define the axis of maximum variance and the eigenvalues give a measure of the variance of the data. \n",
    "\n",
    "1. Construct the vector in the (m x n)-dimensional space R given M images of size (m x n).\n",
    "\n",
    "2. Compute the mean image \n",
    "\n",
    "<center>\n",
    "$\\overline{X}=\\frac{1}{M}\\sum_{i=1}^{M} X_i$\n",
    "</center>\n",
    "\n",
    "3. Construct the covariance matrix. Due to $A \\times A^T$ is too large, instead of using $A \\times A^T$ to compute its eigenvectors, we are going to compute the eigenvectors of $A^T \\times A$.\n",
    "\n",
    "<img src=\"notebook_images/covariance_image.PNG\" width=500, height=500>\n",
    "\n",
    "4. Extract the eigenvectors (the base of the new space) and their eigenvalues and project faces in the new space to apply the classifier (knn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load and prepare data\n",
    "\n",
    "Let's use the [Labeled Faces in the Wild (LFW)](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_lfw_people.html) people dataset (classification).\n",
    "\n",
    "Face dataset features:\n",
    "\n",
    "- Classes: 5749\n",
    "\n",
    "- Samples total: 13233\n",
    "\n",
    "- Dimensionality: 5828\n",
    "\n",
    "- Features: real, between 0 and 255\n",
    "\n",
    "\n",
    "*sklearn.datasets.fetch_lfw_people(data_home=None, funneled=True, resize=0.5, min_faces_per_person=0, color=False, slice_=(slice(70, 195, None), slice(78, 172, None)), download_if_missing=True, return_X_y=False)*\n",
    "\n",
    "\n",
    "\n",
    "*Please, check the parameters and returned value by ``lethc_lfw_people()`` before continuing.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.1** Load the dataset.\n",
    "\n",
    "Obtain only those cases where there are, at least, 100 images. Check the final number of images, image shapes and labels of the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "from sklearn.datasets import fetch_lfw_people \n",
    "\n",
    "dataset = fetch_lfw_people(data_home=None, funneled=True, resize=0.5, \n",
    "                                                     min_faces_per_person=100, color=False, \n",
    "                                                     slice_=(slice(70, 195, None), slice(78, 172, None)), \n",
    "                                                     download_if_missing=True, return_X_y=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data': array([[ 76.       ,  82.666664 ,  91.333336 , ...,  34.       ,\n",
       "          31.333334 ,  28.333334 ],\n",
       "        [ 40.333332 ,  48.666668 ,  82.       , ..., 119.       ,\n",
       "         210.33333  , 240.66667  ],\n",
       "        [ 50.666668 ,  63.333332 ,  81.666664 , ..., 252.33333  ,\n",
       "         251.66667  , 250.33333  ],\n",
       "        ...,\n",
       "        [ 73.333336 ,  78.666664 ,  84.333336 , ...,  73.       ,\n",
       "          57.333332 ,  54.666668 ],\n",
       "        [102.333336 , 102.333336 , 101.666664 , ..., 235.33333  ,\n",
       "         230.33333  , 226.33333  ],\n",
       "        [  1.3333334,   1.6666666,   3.3333333, ...,  64.333336 ,\n",
       "          33.333332 ,   7.       ]], dtype=float32),\n",
       " 'images': array([[[ 76.       ,  82.666664 ,  91.333336 , ..., 113.333336 ,\n",
       "           92.333336 ,  97.333336 ],\n",
       "         [ 74.666664 ,  84.333336 ,  94.666664 , ..., 111.333336 ,\n",
       "           94.666664 ,  95.       ],\n",
       "         [ 79.       ,  90.666664 ,  98.666664 , ..., 114.333336 ,\n",
       "          101.       ,  99.333336 ],\n",
       "         ...,\n",
       "         [ 14.666667 ,  16.       ,  18.666666 , ...,  38.       ,\n",
       "           34.       ,  30.       ],\n",
       "         [ 14.666667 ,  15.666667 ,  17.666666 , ...,  37.       ,\n",
       "           31.666666 ,  28.333334 ],\n",
       "         [ 15.333333 ,  15.333333 ,  17.       , ...,  34.       ,\n",
       "           31.333334 ,  28.333334 ]],\n",
       " \n",
       "        [[ 40.333332 ,  48.666668 ,  82.       , ...,  34.       ,\n",
       "           28.333334 ,  24.333334 ],\n",
       "         [ 62.       ,  73.333336 ,  98.       , ...,  53.333332 ,\n",
       "           45.666668 ,  38.666668 ],\n",
       "         [ 92.       ,  75.       , 101.       , ...,  69.333336 ,\n",
       "           66.333336 ,  55.666668 ],\n",
       "         ...,\n",
       "         [  4.       ,   4.3333335,   3.6666667, ...,  51.       ,\n",
       "           59.333332 , 118.       ],\n",
       "         [  3.3333333,   3.6666667,   3.       , ...,  54.666668 ,\n",
       "          130.66667  , 209.66667  ],\n",
       "         [  3.3333333,   3.       ,   3.       , ..., 119.       ,\n",
       "          210.33333  , 240.66667  ]],\n",
       " \n",
       "        [[ 50.666668 ,  63.333332 ,  81.666664 , ...,  76.       ,\n",
       "           70.333336 ,  73.       ],\n",
       "         [ 45.333332 ,  59.666668 ,  81.       , ...,  84.       ,\n",
       "           75.333336 ,  80.       ],\n",
       "         [ 38.666668 ,  55.333332 ,  80.666664 , ...,  97.333336 ,\n",
       "           84.333336 ,  87.       ],\n",
       "         ...,\n",
       "         [ 48.333332 ,  47.666668 ,  46.666668 , ..., 249.66667  ,\n",
       "          250.33333  , 250.33333  ],\n",
       "         [ 46.666668 ,  46.666668 ,  46.666668 , ..., 251.66667  ,\n",
       "          252.       , 250.33333  ],\n",
       "         [ 46.333332 ,  46.666668 ,  46.666668 , ..., 252.33333  ,\n",
       "          251.66667  , 250.33333  ]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[ 73.333336 ,  78.666664 ,  84.333336 , ..., 155.       ,\n",
       "          149.66667  , 120.333336 ],\n",
       "         [ 70.       ,  74.       ,  82.333336 , ..., 160.33333  ,\n",
       "          158.       , 131.66667  ],\n",
       "         [ 68.333336 ,  73.       ,  85.       , ..., 167.       ,\n",
       "          162.       , 141.66667  ],\n",
       "         ...,\n",
       "         [190.33333  , 190.33333  , 190.66667  , ...,  54.       ,\n",
       "           54.333332 ,  54.666668 ],\n",
       "         [186.66667  , 189.66667  , 191.       , ...,  65.333336 ,\n",
       "           57.       ,  55.666668 ],\n",
       "         [182.66667  , 189.66667  , 194.       , ...,  73.       ,\n",
       "           57.333332 ,  54.666668 ]],\n",
       " \n",
       "        [[102.333336 , 102.333336 , 101.666664 , ...,  87.666664 ,\n",
       "           78.666664 ,  58.666668 ],\n",
       "         [106.333336 , 107.       , 103.333336 , ..., 103.333336 ,\n",
       "           85.333336 ,  64.666664 ],\n",
       "         [108.666664 , 111.666664 , 107.666664 , ..., 109.666664 ,\n",
       "           95.666664 ,  77.       ],\n",
       "         ...,\n",
       "         [111.333336 , 111.666664 , 110.       , ..., 199.66667  ,\n",
       "          229.33333  , 231.33333  ],\n",
       "         [109.333336 , 109.       , 106.666664 , ..., 233.66667  ,\n",
       "          233.33333  , 230.       ],\n",
       "         [107.       , 102.666664 ,  97.       , ..., 235.33333  ,\n",
       "          230.33333  , 226.33333  ]],\n",
       " \n",
       "        [[  1.3333334,   1.6666666,   3.3333333, ...,  57.       ,\n",
       "           47.666668 ,  37.333332 ],\n",
       "         [  1.3333334,   2.       ,   3.3333333, ...,  65.666664 ,\n",
       "           65.666664 ,  60.333332 ],\n",
       "         [  2.       ,   2.3333333,   3.6666667, ...,  66.       ,\n",
       "           77.666664 ,  86.333336 ],\n",
       "         ...,\n",
       "         [  5.3333335,   5.6666665,   5.       , ...,  76.666664 ,\n",
       "           50.       ,  15.666667 ],\n",
       "         [  5.6666665,   6.       ,   6.       , ...,  70.666664 ,\n",
       "           41.333332 ,  11.       ],\n",
       "         [  6.       ,   6.       ,   6.       , ...,  64.333336 ,\n",
       "           33.333332 ,   7.       ]]], dtype=float32),\n",
       " 'target': array([0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "        1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "        1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "        0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "        0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "        1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1,\n",
       "        0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0,\n",
       "        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "        1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0,\n",
       "        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1,\n",
       "        1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "        0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1,\n",
       "        0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0,\n",
       "        0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0,\n",
       "        0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "        1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0,\n",
       "        1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "        1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "        0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "        0]),\n",
       " 'target_names': array(['George W Bush', 'Gerhard Schroeder'], dtype='<U17'),\n",
       " 'DESCR': \".. _labeled_faces_in_the_wild_dataset:\\n\\nThe Labeled Faces in the Wild face recognition dataset\\n------------------------------------------------------\\n\\nThis dataset is a collection of JPEG pictures of famous people collected\\nover the internet, all details are available on the official website:\\n\\n    http://vis-www.cs.umass.edu/lfw/\\n\\nEach picture is centered on a single face. The typical task is called\\nFace Verification: given a pair of two pictures, a binary classifier\\nmust predict whether the two images are from the same person.\\n\\nAn alternative task, Face Recognition or Face Identification is:\\ngiven the picture of the face of an unknown person, identify the name\\nof the person by referring to a gallery of previously seen pictures of\\nidentified persons.\\n\\nBoth Face Verification and Face Recognition are tasks that are typically\\nperformed on the output of a model trained to perform Face Detection. The\\nmost popular model for Face Detection is called Viola-Jones and is\\nimplemented in the OpenCV library. The LFW faces were extracted by this\\nface detector from various online websites.\\n\\n**Data Set Characteristics:**\\n\\n    =================   =======================\\n    Classes                                5749\\n    Samples total                         13233\\n    Dimensionality                         5828\\n    Features            real, between 0 and 255\\n    =================   =======================\\n\\nUsage\\n~~~~~\\n\\n``scikit-learn`` provides two loaders that will automatically download,\\ncache, parse the metadata files, decode the jpeg and convert the\\ninteresting slices into memmapped numpy arrays. This dataset size is more\\nthan 200 MB. The first load typically takes more than a couple of minutes\\nto fully decode the relevant part of the JPEG files into numpy arrays. If\\nthe dataset has  been loaded once, the following times the loading times\\nless than 200ms by using a memmapped version memoized on the disk in the\\n``~/scikit_learn_data/lfw_home/`` folder using ``joblib``.\\n\\nThe first loader is used for the Face Identification task: a multi-class\\nclassification task (hence supervised learning)::\\n\\n  >>> from sklearn.datasets import fetch_lfw_people\\n  >>> lfw_people = fetch_lfw_people(min_faces_per_person=70, resize=0.4)\\n\\n  >>> for name in lfw_people.target_names:\\n  ...     print(name)\\n  ...\\n  Ariel Sharon\\n  Colin Powell\\n  Donald Rumsfeld\\n  George W Bush\\n  Gerhard Schroeder\\n  Hugo Chavez\\n  Tony Blair\\n\\nThe default slice is a rectangular shape around the face, removing\\nmost of the background::\\n\\n  >>> lfw_people.data.dtype\\n  dtype('float32')\\n\\n  >>> lfw_people.data.shape\\n  (1288, 1850)\\n\\n  >>> lfw_people.images.shape\\n  (1288, 50, 37)\\n\\nEach of the ``1140`` faces is assigned to a single person id in the ``target``\\narray::\\n\\n  >>> lfw_people.target.shape\\n  (1288,)\\n\\n  >>> list(lfw_people.target[:10])\\n  [5, 6, 3, 1, 0, 1, 3, 4, 3, 0]\\n\\nThe second loader is typically used for the face verification task: each sample\\nis a pair of two picture belonging or not to the same person::\\n\\n  >>> from sklearn.datasets import fetch_lfw_pairs\\n  >>> lfw_pairs_train = fetch_lfw_pairs(subset='train')\\n\\n  >>> list(lfw_pairs_train.target_names)\\n  ['Different persons', 'Same person']\\n\\n  >>> lfw_pairs_train.pairs.shape\\n  (2200, 2, 62, 47)\\n\\n  >>> lfw_pairs_train.data.shape\\n  (2200, 5828)\\n\\n  >>> lfw_pairs_train.target.shape\\n  (2200,)\\n\\nBoth for the :func:`sklearn.datasets.fetch_lfw_people` and\\n:func:`sklearn.datasets.fetch_lfw_pairs` function it is\\npossible to get an additional dimension with the RGB color channels by\\npassing ``color=True``, in that case the shape will be\\n``(2200, 2, 62, 47, 3)``.\\n\\nThe :func:`sklearn.datasets.fetch_lfw_pairs` datasets is subdivided into\\n3 subsets: the development ``train`` set, the development ``test`` set and\\nan evaluation ``10_folds`` set meant to compute performance metrics using a\\n10-folds cross validation scheme.\\n\\n.. topic:: References:\\n\\n * `Labeled Faces in the Wild: A Database for Studying Face Recognition\\n   in Unconstrained Environments.\\n   <http://vis-www.cs.umass.edu/lfw/lfw.pdf>`_\\n   Gary B. Huang, Manu Ramesh, Tamara Berg, and Erik Learned-Miller.\\n   University of Massachusetts, Amherst, Technical Report 07-49, October, 2007.\\n\\n\\nExamples\\n~~~~~~~~\\n\\n:ref:`sphx_glr_auto_examples_applications_plot_face_recognition.py`\\n\"}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset #conte: dades, imatges, target(classes i coses), description (noms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.2** Plot an image frome each example, with its name as the title of the image.\n",
    "\n",
    "<img src=\"notebook_images/example.png\" width=500, height=500>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = dataset['images'][:5]\n",
    "labels = dataset['target'][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.3** Divide the dataset into train and test set (0.7/0.3). \n",
    "\n",
    "**Hint:** use the train_test_split function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[[ 76.       ,  82.666664 ,  91.333336 , ..., 113.333336 ,\n",
       "           92.333336 ,  97.333336 ],\n",
       "         [ 74.666664 ,  84.333336 ,  94.666664 , ..., 111.333336 ,\n",
       "           94.666664 ,  95.       ],\n",
       "         [ 79.       ,  90.666664 ,  98.666664 , ..., 114.333336 ,\n",
       "          101.       ,  99.333336 ],\n",
       "         ...,\n",
       "         [ 14.666667 ,  16.       ,  18.666666 , ...,  38.       ,\n",
       "           34.       ,  30.       ],\n",
       "         [ 14.666667 ,  15.666667 ,  17.666666 , ...,  37.       ,\n",
       "           31.666666 ,  28.333334 ],\n",
       "         [ 15.333333 ,  15.333333 ,  17.       , ...,  34.       ,\n",
       "           31.333334 ,  28.333334 ]],\n",
       " \n",
       "        [[ 40.333332 ,  48.666668 ,  82.       , ...,  34.       ,\n",
       "           28.333334 ,  24.333334 ],\n",
       "         [ 62.       ,  73.333336 ,  98.       , ...,  53.333332 ,\n",
       "           45.666668 ,  38.666668 ],\n",
       "         [ 92.       ,  75.       , 101.       , ...,  69.333336 ,\n",
       "           66.333336 ,  55.666668 ],\n",
       "         ...,\n",
       "         [  4.       ,   4.3333335,   3.6666667, ...,  51.       ,\n",
       "           59.333332 , 118.       ],\n",
       "         [  3.3333333,   3.6666667,   3.       , ...,  54.666668 ,\n",
       "          130.66667  , 209.66667  ],\n",
       "         [  3.3333333,   3.       ,   3.       , ..., 119.       ,\n",
       "          210.33333  , 240.66667  ]],\n",
       " \n",
       "        [[ 69.333336 ,  70.       ,  82.666664 , ...,  62.666668 ,\n",
       "           81.       , 112.       ],\n",
       "         [ 71.333336 ,  72.       ,  86.333336 , ...,  66.333336 ,\n",
       "           93.333336 ,  97.666664 ],\n",
       "         [ 73.666664 ,  76.666664 ,  92.       , ...,  65.333336 ,\n",
       "          102.       , 103.       ],\n",
       "         ...,\n",
       "         [155.66667  , 164.       , 178.       , ...,   7.3333335,\n",
       "            8.333333 ,  10.       ],\n",
       "         [131.33333  , 143.33333  , 157.66667  , ...,   7.3333335,\n",
       "            9.       ,  10.333333 ],\n",
       "         [ 92.       , 100.333336 , 122.       , ...,   6.       ,\n",
       "            9.       ,  10.       ]]], dtype=float32),\n",
       " array([[[ 50.666668,  63.333332,  81.666664, ...,  76.      ,\n",
       "           70.333336,  73.      ],\n",
       "         [ 45.333332,  59.666668,  81.      , ...,  84.      ,\n",
       "           75.333336,  80.      ],\n",
       "         [ 38.666668,  55.333332,  80.666664, ...,  97.333336,\n",
       "           84.333336,  87.      ],\n",
       "         ...,\n",
       "         [ 48.333332,  47.666668,  46.666668, ..., 249.66667 ,\n",
       "          250.33333 , 250.33333 ],\n",
       "         [ 46.666668,  46.666668,  46.666668, ..., 251.66667 ,\n",
       "          252.      , 250.33333 ],\n",
       "         [ 46.333332,  46.666668,  46.666668, ..., 252.33333 ,\n",
       "          251.66667 , 250.33333 ]],\n",
       " \n",
       "        [[ 22.333334,  26.      ,  36.      , ...,  41.666668,\n",
       "           32.666668,  34.333332],\n",
       "         [ 26.666666,  32.666668,  51.333332, ...,  56.333332,\n",
       "           36.666668,  37.333332],\n",
       "         [ 35.333332,  45.333332,  71.333336, ...,  77.      ,\n",
       "           44.      ,  39.666668],\n",
       "         ...,\n",
       "         [ 49.666668,  94.666664, 141.66667 , ...,  76.333336,\n",
       "           55.      ,  57.      ],\n",
       "         [ 49.666668,  97.333336, 138.66667 , ...,  78.666664,\n",
       "           55.333332,  56.666668],\n",
       "         [ 50.      , 100.666664, 141.33333 , ...,  81.      ,\n",
       "           55.      ,  56.      ]]], dtype=float32),\n",
       " array([0, 0, 0]),\n",
       " array([1, 1]))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split dataset into training and test\n",
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(images, labels)\n",
    "\n",
    "X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Compute PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[*class sklearn.decomposition.PCA(n_components=None, copy=True, whiten=False, svd_solver='auto', tol=0.0, iterated_power='auto', random_state=None)*](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html?highlight=pca#sklearn.decomposition.PCA)\n",
    "\n",
    "The principal components measure deviations about this mean along orthogonal axes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.1** Create a PCA object, using the training set and a 150 components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Principal component analysis\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "PCA(class sklearn.decomposition.PCA(n_components=None, copy=True, whiten=False, svd_solver='auto', tol=0.0,\n",
    "                                    iterated_power='auto', random_state=None))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the accumated variance of the components. \n",
    "\n",
    "**Hint:** Use the returned `explained_variance_ratio_`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the variable *pca*?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.2** One interesting part of PCA is that it computes the average face, which can be interesting to examine. \n",
    "\n",
    "Plot the average face, using the method `mean_` of the PCA object.\n",
    "\n",
    "**Hint:** The average face need to be reshaped in order to visualize it properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.3** Furhtermore, we can check all the principal components (i.e. eigenfaces) considering the corresponding importance.\n",
    "\n",
    "Visualize 30 principal eigenfaces\n",
    "\n",
    "<img src=\"notebook_images/eigenfaces.png\" width=500, height=500>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the base components are ordered by their importance. We see that the first few components seem to primarily take care of lighting conditions; the remaining components pull out certain identifying features: the nose, eyes, eyebrows, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.4** Project both the training and test set onto the PCA basis, using the method `transform()` of the PCA object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DO you need to apply the same to the variable y?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.5**  These projected components correspond to factors in a linear combination of component images such that the combination approaches the original face. \n",
    "\n",
    "Choose one of the images and try to recompose from its first 10 most important corresponding eigenfaces."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<br/>\n",
    "<img src=\"notebook_images/eigenfaces_image.PNG\" width=300, height=300>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the purpose of the PCA tool?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Train a classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.1** Train an Adaboost classifier using the PCA features. Show the results obtained with the test set.\n",
    "Use the `score` method of the Adaboost model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a Adaboost classifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "# Your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.2** We can quantify this effectiveness using one of several measures from sklearn.metrics. First we can do the classification report, which shows the precision, recall and other measures of the “goodness” of the classification.\n",
    "\n",
    "*sklearn.metrics.classification_report(y_true, y_pred, labels=None, target_names=None, sample_weight=None, digits=2, output_dict=False, zero_division='warn')*\n",
    "\n",
    "*Please, check the parameters and returned value by ``classification_report()`` before continuing.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the classification report obtained during the training of the Adaboost model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification and results\n",
    "from sklearn.metrics import classification_report \n",
    "\n",
    "# Your solution here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please, explaing what is:\n",
    "- accuracy (score)\n",
    "- precision\n",
    "- recall\n",
    "- f1-score\n",
    "- support\n",
    "- macro avg\n",
    "- weighted avg?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your explanation here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.3** Train an Adaboost classifier, without PCA, using the training set. Show the results using the `score` method of the Adaboost model and the corresponding classification report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a Adaboost classifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "# Your solution here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Questions**\n",
    "\n",
    "- Does computing time change using PCA? How?\n",
    "- Which of both (with and without PCA) does give better results?\n",
    "- How does the result change if we change the number of components in PCA?\n",
    "- How does the result change if we change the number of estimators in the Adaboost?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.5 (Optional)** Repeat the experiment using a different number of components. \n",
    "\n",
    "For instance, instead of using 150 components, try using 10, 25, 50, 200, 500... at your election. These numbers are just orientative. \n",
    "\n",
    "- How much variance is acummulated using the different number of components.\n",
    "- The result is better using... how many components? \n",
    "- Does time change using a different numbero of components?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Recognize a new face example using the learned model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.1** Try your both models using the test set. \n",
    "\n",
    "Predict the labels using the Adaboost model, with and without PCA, and plot the images with the corresponding label as title.\n",
    "\n",
    "<img src=\"notebook_images/prediction.png\" width=300 height = 300>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which model makes the predictions betters? Try different parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
