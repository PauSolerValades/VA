{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Funcions útils i recurrents.\n",
    "- visualize_n: es pot fer servir per només una imatge però l'has de posar en una llista d'un element.\n",
    "- normalize: normalitza una imatge, independentment estigui en float o en int."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_n(image_list):\n",
    "\n",
    "    fig=plt.figure(figsize=(15,15))\n",
    "    for i in range(len(image_list)):\n",
    "        \n",
    "        fig.add_subplot(1,len(image_list),i +1)\n",
    "        plt.imshow(image_list[i], cmap='gray')\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        \n",
    "def visualize_n_w_labels(image_list, labels):\n",
    "\n",
    "    fig=plt.figure(figsize=(15,15))\n",
    "    for i in range(len(image_list)):\n",
    "        \n",
    "        fig.add_subplot(1,len(image_list),i +1)\n",
    "        plt.imshow(image_list[i], cmap='gray')\n",
    "        plt.title(labels[i])\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "\n",
    "def normalize(image): return (image[:,:]- image.min())/(image.max() - image.min())\n",
    "\n",
    "def normalize_array(array): return (array[:]- array.min())/(array.max() - array.min())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Template Matching\n",
    "\n",
    "Tenim la SSD (sum square difference), NCC (normalized cross correlation) i els dos thresholds per trobar-ne el màxim (SSD) i el mínim (NCC) que son respectivament els punts que busquem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_squares_difference(image, pattern): \n",
    "    \n",
    "    pattern = normalize(pattern)\n",
    "    image = normalize(image)\n",
    "    \n",
    "    x = image.shape[0]\n",
    "    y = image.shape[1]\n",
    "    i = pattern.shape[0]\n",
    "    j = pattern.shape[1]\n",
    "    \n",
    "    return normalize(np.array([[np.linalg.norm(np.linalg.norm(pattern-image[n:n+i,m:m+j])) \n",
    "                                for m in range(0, y-j)] for n in range(0,x-i)]))\n",
    "\n",
    "#NORMALIZED CROSS CORRELATION: \n",
    "#match_template(image, pattern)\n",
    "\n",
    "def mask_ssd(image, threshold):\n",
    "    mask = np.ones(image.shape)\n",
    "    mask[image > threshold] = 0 #el mínim és el millor match\n",
    "    return mask\n",
    "\n",
    "def mask_ncc(image, threshold):\n",
    "    mask = np.ones(image.shape)\n",
    "    image = normalize(image)\n",
    "    mask[image <= image.max() - threshold] = 0 #el màxim és el millor match\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NO son invariants:\n",
    "- Contrast: canviar el valor numeric de la intensitat afecta directament al que estem intentant calcular\n",
    "- Rotació: girar el patró fa que estiguem buscant un patró diferent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 HOG\n",
    "\n",
    "El hog l'extreu skimage amb la següent funció, on el primer parametre és l'actual HOG i el segon és l'imatge que forma el HOG descriptor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'io' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-1bcb634ec808>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mCPB\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mperson\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'images/person_template.bmp'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m HOG_descriptor, hog_image = hog(person, orientations=8, pixels_per_cell=PPC,\n",
      "\u001b[0;31mNameError\u001b[0m: name 'io' is not defined"
     ]
    }
   ],
   "source": [
    "from skimage.feature import hog\n",
    "\n",
    "PPC = (4,4) #píxels per cell\n",
    "CPB = (2,2) #cell per block\n",
    "\n",
    "image = io.imread('images/person_template.bmp')\n",
    "\n",
    "HOG_descriptor, hog_image = hog(image, orientations=8, pixels_per_cell=PPC,\n",
    "                cells_per_block=CPB, visualize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La següent funció compara el HOG d'una imatge i en busca el millor match amb una finestra lliscant (és a dir, la distància mínima entre els descriptors HOG:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hog_distances(fd_pattern, target, pattern):\n",
    "    \n",
    "    hpat, wpat = pattern.shape[0], pattern.shape[1]\n",
    "    \n",
    "    distances = np.zeros((target.shape[0], target.shape[1]))\n",
    "    height,width = target.shape[0], target.shape[1]\n",
    "    \n",
    "    for i in range(0, height-hpat, 5):\n",
    "        for j in range(0, width-wpat, 5):\n",
    "            \n",
    "            region = image[i:i+hpat, j:j+wpat]\n",
    "            fd_region, hog_region = hog(region, orientations=8, pixels_per_cell=PPC, \n",
    "                                        cells_per_block=CPB, visualize=True)\n",
    "            distances[i+round(hpat/2),j+round(wpat/2)]=np.dot(fd_pattern,fd_region)\n",
    "            \n",
    "    return distances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dondada una imatge, aquesta funció calcula la dimensió del descriptor hog:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dimension_HOG(image_shape, CPB, orientation):\n",
    "    histogramsPerBlock = CPB[0]*CPB[1]*orientation\n",
    "    \n",
    "    horitzontalBlocks = nearest_divisor(image_shape[0],4)/4-1\n",
    "    verticalBlocks = nearest_divisor(image_shape[1],4)/4-1\n",
    "    numBlocks = horitzontalBlocks*verticalBlocks\n",
    "    \n",
    "    return int(numBlocks*histogramsPerBlock)\n",
    "\n",
    "dimension_HOG(person.shape, CPB, orientation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "És invariant pel contrast i per rotacions, ja que el gradient es manté igual. No ens fa falta un threshold així que la puta hostia bro."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 ORB\n",
    "\n",
    "Podem detectar punts d'interès amb CENSURE i les següents funcions. La visualització es fa amb una funció especial que et donen a les pràctiques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import data\n",
    "from skimage import transform\n",
    "from skimage.feature import CENSURE\n",
    "from skimage.color import rgb2gray\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import io\n",
    "\n",
    "image = io.imread('images/starbucks4.jpg')\n",
    "img_gr = rgb2gray(image)\n",
    "detector1 = CENSURE()\n",
    "detector1.detect(img_gr)\n",
    "\n",
    "detector2 = CENSURE(non_max_threshold=0.05)\n",
    "detector2.detect(img_gr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per els punts d'interès amb ORB utilitzem la funció get_ORB. Plot_matches ens dibuixa les línies entre dues:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'io' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-5d538c9169b3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m \u001b[0mimage_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"images/starbucks.jpg\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0mkeypoints1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeypoints2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_ORB\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_keypoints\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m45\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_ratio\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'io' is not defined"
     ]
    }
   ],
   "source": [
    "from skimage.feature import (match_descriptors, corner_harris,\n",
    "                             corner_peaks, ORB, plot_matches)\n",
    "\n",
    "def get_ORB(image1, image2, n_keypoints=50, max_ratio=1.0):\n",
    "    \n",
    "    image1 = rgb2gray(image1)\n",
    "    image2 = rgb2gray(image2)\n",
    "    \n",
    "    descriptor_extractor = ORB(n_keypoints=n_keypoints) #això et fa tot orb xd\n",
    "\n",
    "    descriptor_extractor.detect_and_extract(image1) #\n",
    "    keypoints1 = descriptor_extractor.keypoints\n",
    "    descriptors1 = descriptor_extractor.descriptors\n",
    "\n",
    "    descriptor_extractor.detect_and_extract(image2)\n",
    "    keypoints2 = descriptor_extractor.keypoints\n",
    "    descriptors2 = descriptor_extractor.descriptors\n",
    "    \n",
    "    #match_descriptors et diu quins son els millors matches dels descriptors, tabmé skimage\n",
    "    matches12 = match_descriptors(descriptors1, descriptors2, cross_check=True, max_ratio=max_ratio)\n",
    "    \n",
    "    return (keypoints1, keypoints2, matches12)\n",
    "\n",
    "def plot_matches(image1, image2, keypoints1, keypoints2, matches):\n",
    "    fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(15,15))\n",
    "    plt.gray()\n",
    "    plot_matches_aux(ax, image1, image2, keypoints1, keypoints2, matches)\n",
    "    ax.axis('off')\n",
    "\n",
    "    plt.show()\n",
    "    return\n",
    "\n",
    "\n",
    "image_model = io.imread(\"images/starbucks.jpg\")\n",
    "\n",
    "keypoints1, keypoints2, matches = get_ORB(image_model, image, n_keypoints=45, max_ratio=1.5)\n",
    "plot_matches(image_model, image, keypoints1, keypoints2, matches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una imatge és més similar (té un millor score) només depen del nombre de matches que trobi la funció. És a dir, a més gran el cardinal de matches, més similar és. A sota exemple d'ordenar per matches les imatges:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute siliarity for every image, it's defined for number of matches\n",
    "similar = []\n",
    "for image in images:\n",
    "    keypoints1, keypoints2, matches = get_ORB(image_model, image, n_keypoints=50)\n",
    "    similar.append((image, len(matches), keypoints1, keypoints2, matches))\n",
    "    \n",
    "similar.sort(key=lambda x:x[1], reverse=True) #Sort by similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A priori ORB és invariant per:\n",
    "- Rotació\n",
    "- Escala\n",
    "- Es pot fer in real time\n",
    "Encara que al notebook doncs no ens hagi sortit tan bé hhahaha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 Textures\n",
    "\n",
    "És quasi millor mirar el notebook 5.2 que directament es parla de similitud. Aquí només es parla de com preparar els nombres.\n",
    "\n",
    "1. Resize TOTES LES IMATGES al mateix tamany.\n",
    "2. Convertir a escala de grisos\n",
    "\n",
    "Les següents funcions serveixen per trobar les features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_filters(im,  filters): #fa les convolucions entre els filtres i la imatge\n",
    "    return np.array([np.abs(convolve(im, f)) for f in filters])\n",
    "\n",
    "#extreu les features amb la mitjana dels valors absoluts de cada imatge.\n",
    "def extract_features(image, filter_bank, n_filters):\n",
    "    \n",
    "    filters = bank[0:n_filters]\n",
    "    filtered_images = apply_filters(image, filters)  \n",
    "    \n",
    "    return np.array([np.mean(np.abs(f_image.flatten())) for f_image in filtered_images])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Després et demana treure les features de tot el dataset (aplicar extract_features a totes les imatges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset_features(all_images, bank):\n",
    "    n_filters = len(bank)\n",
    "    \n",
    "    result = [extract_features(image, bank, n_filters) for image in all_images]\n",
    "    \n",
    "    return np.array(result)\n",
    "\n",
    "#feature_vectors=get_dataset_features(all_images_gr, bank)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per trobar la imatge més similar del dataset només hem de restar les features de la nostra imatge amb totes i cadascuna d'elles i trobar la resta més petita:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'feature_vectors' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-9cc1c3a94f74>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdists\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mimage_feature\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfeature_vectors\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mclosest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdists\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdists\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#ajunta les distancies amb el seu index per saber quina era\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mclosest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#ordena segons les distancies\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'feature_vectors' is not defined"
     ]
    }
   ],
   "source": [
    "dists = [np.linalg.norm((feature - image_feature)) for feature in feature_vectors]\n",
    "closest = list(zip(range(len(dists)), dists)) #ajunta les distancies amb el seu index per saber quina era\n",
    "closest.sort(key=lambda x:x[1]) #ordena segons les distancies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculem l'accuracy com la divisió entre el nombre de classes correctes entre el total d'imatges que ens ha tornat el model. És a dir si entro una pizza per les 10 més properes, i m'ha tornat 7 pizzes, 2 gats i una flor, l'accuracy és 7/10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(feature_vectors, class_labels, im_features, class_im, n_features, k=5):\n",
    "    \n",
    "    im_features = im_features[:n_features]\n",
    "\n",
    "    dists = [np.linalg.norm((feature[:n_features] - im_features)) for feature in feature_vectors]\n",
    "    closest = list(zip(range(len(dists)), dists))\n",
    "    closest.sort(key=lambda x:x[1])\n",
    "    \n",
    "    closest_labels = [class_labels[i[0]] for i in closest[:k] if i[1] != 0.0] #Take labels of k closest images\n",
    "    \n",
    "    trues = [class_im==i for i in closest_labels] #List of matches with expected label\n",
    "\n",
    "    return np.count_nonzero(np.array(trues))/len(closest_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aquest procés es pot ampliar tenint en compte els colors (no crec que ho pregunti, està al notebook de la practica que es pot consultar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 Haar-Like features\n",
    "\n",
    "Primer, l'integral image d'una imatge és una funció propia d'skimage, així que no et rallis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.transform import integral_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les haar-like features es criden amb les següents funcions. feature_type ha de ser una llista, però pot tenir només un o dos element. En aquest cas, et donarà diferens tipus de quadrats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_types = ['type-2-x', 'type-2-y',\n",
    "                 'type-3-x', 'type-3-y',\n",
    "                 'type-4']\n",
    "\n",
    "def extract_feature_image(image, feature_type, feature_coord=None):\n",
    "    int_image = integral_image(image)\n",
    "    return haar_like_feature(int_image, 0,0, image.shape[0], image.shape[1], feature_type=feature_type, \n",
    "                             feature_coord=feature_coord)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per visualitzar-les usem plotFeatures. haar_like_feature_coord() és una llista amb totes les coordenades on començen els diferents filtres. Al for anem guardant cada imatge amb el haarlike feature dibuixat. La _ és el tipus de filtre que s'ha de dibuixar (el type). Si no l'ometessim podriem filtrar segons aquest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotFeatures(image, num_images=10):\n",
    "\n",
    "    images = []\n",
    "    coord, _ = haar_like_feature_coord(image.shape[0], image.shape[1], feature_type=feature_types)\n",
    "    \n",
    "    for c in coord[:num_images]:\n",
    "        images.append(draw_haar_like_feature(image, 0,0, image.shape[0], image.shape[1], [c]))\n",
    "        \n",
    "    visualize_n(images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ara ve com entrenar un ADABOOST, que està millor explicat al notebook del que podria fer-ho jo aquí sense utilitzar execució de codi\n",
    "\n",
    "Això sí, els mètodes score i predict no són el mateix:\n",
    "- Score: inputs son les imatges de test i les labels corresponents (surt quan separes les dades). Llavors ell evalua la predicció dividint el nombre de classes dels inputs contra les originals. és a dir, les encertades vs les reals.\n",
    "- predict: li passes un vector de dades i ell et retorna un vector d'igual longitud amb les seves classes. Score usa predict per predir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "És un mètode de machine learning, així que per executar-lo s'ha de partir en train/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-5f5aed87a3af>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "# Split dataset into training and test\n",
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.3)\n",
    "\n",
    "n_comp=150\n",
    "\n",
    "pca = PCA(n_components=n_comp, copy=True, whiten=False, svd_solver='auto', tol=0.0, iterated_power='auto', \n",
    "            random_state=None)\n",
    "\n",
    "vector = pca.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A la variable vector.components_ tenim els eigenvectors (en aquest cas abstraccions de cares que ha detectat la maquina) i els podem extreure amb el següent codi:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eigenvectors = [image.reshape((height, width)) for image in vector.components_]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Llavors, projectant el training/test al PCA amb el codi, podem obtenir els eigenvalues, que son els valors que, juntament amb el eigenvectors, ens donen la millor representació una cara amb subcares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_train = pca.transform(X_train)\n",
    "trans_test = pca.transform(X_test)\n",
    "\n",
    "MOST_IMPORTANT = 10\n",
    "\n",
    "eigenvalues = trans_train[0]\n",
    "\n",
    "average = np.sum(np.array([eigenvalues[i]*eigenvectors[i] for i in range(0, MOST_IMPORTANT)]),axis=0)\n",
    "\n",
    "visualize_n([average])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amb totes les dades, podem entrenar un ADABOOST amb les dades reduides del PCA i que tingui molt bons resultats (ara sí mirar el notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
