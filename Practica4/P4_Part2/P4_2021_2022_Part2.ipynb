{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computer Vision - P4\n",
    "\n",
    "### **Carefully read the file `README.md` as well as the following instructions before start coding.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delivery\n",
    "\n",
    "Up to **1 point out of 10** will be penalized if the following requirements are not fulfilled:\n",
    "\n",
    "- Implemented code should be commented.\n",
    "\n",
    "- The questions introduced in the exercises must be answered.\n",
    "\n",
    "- Add title to the figures to explain what is displayed.\n",
    "\n",
    "- Comments and answers need to be in **english**.\n",
    "\n",
    "- The deliverable must be a file named **P4_Student1_Student2.zip** that includes:\n",
    "    - The notebook P4_Student1_Student2.ipynb completed with the solutions to the exercises and their corresponding comments.\n",
    "\n",
    "**Deadline (Campus Virtual): November 16th, 23:00 h** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "==============================================================================================\n",
    "## Descriptors extraction for object detection, based on template matching, ORB, and HOG\n",
    "==============================================================================================\n",
    "\n",
    "The main topics of Laboratory 4 are:\n",
    "\n",
    "**FirstPart: Template Matching and Image Descriptors**\n",
    "\n",
    "    4.1) Euclidean distance and Normalized Cross-correlation for template matching\n",
    "\n",
    "    4.2) HOG image descriptor for object (person) detection\n",
    "\n",
    "**Second Part: Image matching**\n",
    "\n",
    "    4.3) Recognition by correspondance, based on feature extraction (ORB)\n",
    "    \n",
    "In order to complete this practicum, the following concepts need to be understood: template matching, feature localization (Harris, Censure), feature descriptor (HOG,ORB, Sift) methods.\n",
    "\n",
    "It is highly recommendable to structure the code in functions in order to reuse code for different tests and images and make it shorter and more readable. Specially the visualization commands should be encapsulated in separate functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 ORB feature detector and binary descriptor\n",
    "\n",
    "Let us consider the problem of feature extraction that contains two subproblems: \n",
    "- feature location, \n",
    "- image feature description.\n",
    "\n",
    "Let us focus on ORB, an approximation of SIFT method, and analyse if ORB is  scale and rotation invariant, a property that is very important for real-time applications.\n",
    "\n",
    "**Hint:** `ORB` is a function within the module `skimage.feature`\n",
    "                             \n",
    "**Help**: We suggest to have a look at the [ORB example](http://scikit-image.org/docs/dev/auto_examples/features_detection/plot_orb.html) how to compute the ORB descriptors and find the descriptors match. You can use the function match_descriptors from `skimage.feature` module in order to compute and show the similar detected descriptors of the given images. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.1** Detect the censure in the image `starbucks4.jpg`. Analyze and discuss the effect of different values of the parameters in censure function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.2** Detect the correspondences between the model image `starbucks.jpg` with the scene image `starbucks4.jpg`. You can adapt the code from the [ORB example](http://scikit-image.org/docs/dev/auto_examples/features_detection/plot_orb.html) above. \n",
    "\n",
    "Define a function get_ORB implementing the algorithm in order to be able to apply it on different images. Comment the code in detail.\n",
    "\n",
    "**Hint: If the function plot_matches() gives you an error you can use the plot_matches_aux() at the end of this file.**\n",
    "\n",
    "Analyze and discuss the effect of different values of the parameter `max_ratio` in the match_descriptors function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeate the experiment comparing the `starbucks.jpg` image as a model, and showing its matches to all Starbucks images, sorting them based on their similarity to the model. Comment when does the algorithm work better. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.3** Repeate the experiment: \n",
    "- Changing the orientation of the model image by rotating it and comparing it with its original version. Help: you can use the rotate() function from skimage.transform \n",
    "- Change the scale and orientation of the scene image and compare it with the model image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Help:** To do so, you can use the function given below as example:\n",
    "\n",
    "```\n",
    "import transform as tf\n",
    "rotationdegrees = 180\n",
    "img_rotated = tf.rotate(image2transform, rotationdegrees)\n",
    "```\n",
    "\n",
    "or\n",
    "\n",
    "```\n",
    "tform = tf.AffineTransform(scale=(1.2, 1.2), translation=(0, -100))\n",
    "img_transformed = tf.warp(image2transform, tform)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(Optional)** Repeat the experiment (3.1 to 3.3) with a new group of images. You could use Coca-Cola advertisements or from another famous brand, easily to find on internet.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.4** Analysis of the applied techniques and results\n",
    "\n",
    "- What are the advantages of the ORB object detection with respect to the HOG and template object detector?\n",
    "\n",
    "- What would happen if you analyse an image that does not contain the Starbucks logo? \n",
    "\n",
    "- Could you think of ways of defining a quality measure for the correspondance between two images? (no need of implementing it) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appendix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In case the plot_matches() function gives you some problems, you can use the following one:\n",
    "\n",
    "from skimage.util import img_as_float\n",
    "import numpy as np\n",
    "\n",
    "def plot_matches_aux(ax, image1, image2, keypoints1, keypoints2, matches,\n",
    "                 keypoints_color='k', matches_color=None, only_matches=False):\n",
    "    \"\"\"Plot matched features.\n",
    "    Parameters\n",
    "    ----------\n",
    "    ax : matplotlib.axes.Axes\n",
    "        Matches and image are drawn in this ax.\n",
    "    image1 : (N, M [, 3]) array\n",
    "        First grayscale or color image.\n",
    "    image2 : (N, M [, 3]) array\n",
    "        Second grayscale or color image.\n",
    "    keypoints1 : (K1, 2) array\n",
    "        First keypoint coordinates as ``(row, col)``.\n",
    "    keypoints2 : (K2, 2) array\n",
    "        Second keypoint coordinates as ``(row, col)``.\n",
    "    matches : (Q, 2) array\n",
    "        Indices of corresponding matches in first and second set of\n",
    "        descriptors, where ``matches[:, 0]`` denote the indices in the first\n",
    "        and ``matches[:, 1]`` the indices in the second set of descriptors.\n",
    "    keypoints_color : matplotlib color, optional\n",
    "        Color for keypoint locations.\n",
    "    matches_color : matplotlib color, optional\n",
    "        Color for lines which connect keypoint matches. By default the\n",
    "        color is chosen randomly.\n",
    "    only_matches : bool, optional\n",
    "        Whether to only plot matches and not plot the keypoint locations.\n",
    "    \"\"\"\n",
    "\n",
    "    image1 = img_as_float(image1)\n",
    "    image2 = img_as_float(image2)\n",
    "\n",
    "    new_shape1 = list(image1.shape)\n",
    "    new_shape2 = list(image2.shape)\n",
    "\n",
    "    if image1.shape[0] < image2.shape[0]:\n",
    "        new_shape1[0] = image2.shape[0]\n",
    "    elif image1.shape[0] > image2.shape[0]:\n",
    "        new_shape2[0] = image1.shape[0]\n",
    "\n",
    "    if image1.shape[1] < image2.shape[1]:\n",
    "        new_shape1[1] = image2.shape[1]\n",
    "    elif image1.shape[1] > image2.shape[1]:\n",
    "        new_shape2[1] = image1.shape[1]\n",
    "\n",
    "    if new_shape1 != image1.shape:\n",
    "        new_image1 = np.zeros(new_shape1, dtype=image1.dtype)\n",
    "        new_image1[:image1.shape[0], :image1.shape[1]] = image1\n",
    "        image1 = new_image1\n",
    "\n",
    "    if new_shape2 != image2.shape:\n",
    "        new_image2 = np.zeros(new_shape2, dtype=image2.dtype)\n",
    "        new_image2[:image2.shape[0], :image2.shape[1]] = image2\n",
    "        image2 = new_image2\n",
    "\n",
    "    image = np.concatenate([image1, image2], axis=1)\n",
    "\n",
    "    offset = image1.shape\n",
    "\n",
    "    if not only_matches:\n",
    "        ax.scatter(keypoints1[:, 1], keypoints1[:, 0],\n",
    "                   facecolors='none', edgecolors=keypoints_color)\n",
    "        ax.scatter(keypoints2[:, 1] + offset[1], keypoints2[:, 0],\n",
    "                   facecolors='none', edgecolors=keypoints_color)\n",
    "\n",
    "    ax.imshow(image, interpolation='nearest', cmap='gray')\n",
    "    ax.axis((0, 2 * offset[1], offset[0], 0))\n",
    "\n",
    "    for i in range(matches.shape[0]):\n",
    "        idx1 = matches[i, 0]\n",
    "        idx2 = matches[i, 1]\n",
    "\n",
    "        if matches_color is None:\n",
    "            color = np.random.rand(3)\n",
    "        else:\n",
    "            color = matches_color\n",
    "\n",
    "        ax.plot((keypoints1[idx1, 1], keypoints2[idx2, 1] + offset[1]),\n",
    "                (keypoints1[idx1, 0], keypoints2[idx2, 0]),\n",
    "                '-', color=color)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
